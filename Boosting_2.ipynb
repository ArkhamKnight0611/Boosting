{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Q1. Gradient Boosting Regression is a machine learning technique used for regression tasks. It is an ensemble learning method that combines the predictions of several base estimators (usually decision trees) in a sequential manner. In each iteration, a new model is trained to correct the errors made by the previous models. Gradient Boosting Regression optimizes a loss function by gradient descent, where the gradient of the loss function with respect to the prediction is used to update the model parameters."
      ],
      "metadata": {
        "id": "PNIbOLDeO0bi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rj3JvDfkOfN8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class GradientBoostingRegressor:\n",
        "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.learning_rate = learning_rate\n",
        "        self.max_depth = max_depth\n",
        "        self.trees = []\n",
        "        self.train_errors = []\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        predictions = np.full_like(y, np.mean(y))\n",
        "\n",
        "        for _ in range(self.n_estimators):\n",
        "            residuals = y - predictions\n",
        "            tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
        "            tree.fit(X, residuals)\n",
        "            predictions += self.learning_rate * tree.predict(X)\n",
        "            self.trees.append(tree)\n",
        "            train_error = np.mean((y - self.predict(X)) ** 2)\n",
        "            self.train_errors.append(train_error)\n",
        "    def predict(self, X):\n",
        "        predictions = np.zeros(len(X))\n",
        "        for tree in self.trees:\n",
        "            predictions += self.learning_rate * tree.predict(X)\n",
        "        return predictions"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "X, y = make_regression(n_samples=100, n_features=1, noise=0.1, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "model = GradientBoostingRegressor()\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "print(\"R-squared:\", r2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ca7663CPRob",
        "outputId": "aa8cde8e-ca58-438a-f0f5-ed290d25feec"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 31.735482349161565\n",
            "R-squared: 0.9772379183627112\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3. To experiment with different hyperparameters, you can modify the GradientBoostingRegressor class to accept hyperparameters such as learning rate, number of trees, and tree depth. You can then perform grid search or random search to find the best combination of hyperparameters that optimizes the model's performance.\n",
        "\n",
        "Q4. In Gradient Boosting, a weak learner is a simple model that is only slightly better than random guessing. Typically, decision trees with shallow depth are used as weak learners in Gradient Boosting.\n",
        "\n",
        "Q5. The intuition behind the Gradient Boosting algorithm is to iteratively improve the model by adding weak learners (usually decision trees) to correct the errors made by the previous models. It focuses on minimizing the errors or residuals of the previous models in a step-by-step manner.\n",
        "\n",
        "Q6. Gradient Boosting algorithm builds an ensemble of weak learners sequentially. In each iteration, it fits a weak learner to the residuals of the previous predictions and updates the predictions by adding the predictions of the new model multiplied by a learning rate.\n",
        "\n",
        "Q7. The steps involved in constructing the mathematical intuition of the Gradient Boosting algorithm include:\n",
        "\n",
        "Initialize the model with a constant value (often the mean of the target variable).\n",
        "Compute the residuals by subtracting the current predictions from the actual target values.\n",
        "Fit a weak learner (e.g., decision tree) to the residuals.\n",
        "Multiply the predictions of the weak learner by a small learning rate and add them to the current predictions.\n",
        "Repeat steps 2-4 until a predefined number of iterations or until convergence."
      ],
      "metadata": {
        "id": "-8qucd4-PiEG"
      }
    }
  ]
}